{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import resnet18\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader,  TensorDataset\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# http://localhost:8888/notebooks/Documents/AI/scripts/picture_comparison_crop80.ipynbfrom tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "from scipy import misc\n",
    "import imageio\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import keras as keras\n",
    "from keras_unet.utils import plot_patches\n",
    "from keras_unet.utils import get_patches\n",
    "from keras_unet.utils import reconstruct_from_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#paths to folders with images and segmentations\n",
    "mypath_img = r\"C:\\Users\\adoro\\Documents\\AI\\histology pig big/\"\n",
    "mypath = r\"C:\\Users\\adoro\\Documents\\AI\\SegmentationClass/\"\n",
    "\n",
    "\n",
    "filename = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # sample = {\"image\": image, \"label\": label}\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_Images(images_arr):\n",
    "  fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "  axes = axes.flatten()\n",
    "  rand_num = np.random.randint(0, len_dataset, size=5)\n",
    "  for img, ax in zip(images_arr[rand_num], axes):\n",
    "    # img = np.round(img/np.max(img))\n",
    "    ax.imshow(img.astype(int))\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,testloader,device):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels=labels.squeeze(1)\n",
    "        # print(predicted)\n",
    "        # print(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # print(correct / total)\n",
    "  \n",
    "  return correct / total  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches_custom(img_arr, size=256, stride=256):\n",
    "    \"\"\"\n",
    "    Takes single image or array of images and returns\n",
    "    crops using sliding window method.\n",
    "    If stride < size it will do overlapping.\n",
    "    \n",
    "    Args:\n",
    "        img_arr (numpy.ndarray): [description]\n",
    "        size (int, optional): [description]. Defaults to 256.\n",
    "        stride (int, optional): [description]. Defaults to 256.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: [description]\n",
    "        ValueError: [description]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: [description]\n",
    "    \"\"\"    \n",
    "    # check size and stride\n",
    "    if size % stride != 0:\n",
    "        raise ValueError(\"size % stride must be equal 0\")\n",
    "\n",
    "    patches_list = []\n",
    "    overlapping = 0\n",
    "    if stride != size:\n",
    "        overlapping = (size // stride) - 1\n",
    "\n",
    "    if img_arr.ndim == 3:\n",
    "        i_max = img_arr.shape[0] // stride - overlapping\n",
    "        j_max = img_arr.shape[1] // stride - overlapping\n",
    "        for i in range(i_max):\n",
    "            for j in range(j_max):\n",
    "                # print(i*stride, i*stride+size)\n",
    "                # print(j*stride, j*stride+size)\n",
    "                patches_list.append(\n",
    "                    img_arr[\n",
    "                        i * stride : i * stride + size,\n",
    "                        j * stride : j * stride + size\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    elif img_arr.ndim == 4:\n",
    "        i_max = img_arr.shape[1] // stride - overlapping\n",
    "        for im in img_arr:\n",
    "            for i in range(i_max):\n",
    "                for j in range(i_max):\n",
    "                    # print(i*stride, i*stride+size)\n",
    "                    # print(j*stride, j*stride+size)\n",
    "                    patches_list.append(\n",
    "                        im[\n",
    "                            i * stride : i * stride + size,\n",
    "                            j * stride : j * stride + size,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"img_arr.ndim must be equal 3 or 4\")\n",
    "\n",
    "    return np.stack(patches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.float()\n",
    "#name of weights file\n",
    "model.load_state_dict(torch.load('model_weights_bs100_crop80_1A0_5000_5000.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_area = 0\n",
    "area_part = np.zeros((1, len(filename)))\n",
    "area_true = area_part.copy()\n",
    "\n",
    "for image_path in filename[0:len(filename)]:\n",
    "    path = mypath_img+image_path[0:len(image_path)-3]+'tiff'\n",
    "    image_in = imageio.imread(path)\n",
    "    # size=(w_size,h_size)\n",
    "    image = image_in[:,:,0:3]\n",
    "    w_size = image.shape[0]\n",
    "    h_size = image.shape[1]\n",
    "    \n",
    "    path = mypath+image_path\n",
    "    mask = imageio.imread(path)\n",
    "#     mask[:,:,1] = 0*mask[:,:,1] ########################### deleted green zones\n",
    "    mask[:,:,2] = 0*mask[:,:,2]\n",
    "\n",
    "    if  w_size> h_size:\n",
    "        image = np.swapaxes(image,0,1)\n",
    "        tmp = w_size\n",
    "        w_size = h_size\n",
    "        h_size = tmp\n",
    "        \n",
    "    mask_l = mask[0:w_size-1,:,:]\n",
    "    mask_r = mask[1:w_size,:,:]\n",
    "    mask_u = mask[:,0:h_size-1,:]\n",
    "    mask_d = mask[:,1:h_size,:]\n",
    "    mask_contour = abs(mask_r[:,0:h_size-1,:]-mask_l[:,0:h_size-1,:])+abs(mask_d[0:w_size-1,:,:]-mask_u[0:w_size-1,:,:])    \n",
    "    \n",
    "    half_width = 3\n",
    "    for k in range(-half_width, half_width):\n",
    "        mask_contour[half_width:-half_width,:,:] =  mask_contour[half_width:-half_width,:,:]+mask_contour[half_width+k:-half_width+k,:,:]\n",
    "        mask_contour[:, half_width:-half_width,:] =  mask_contour[:, half_width:-half_width,:]+mask_contour[:, half_width+k:-half_width+k,:]\n",
    "\n",
    "    mask_contour = (255*abs(1-np.sign(mask_contour))).astype(int)\n",
    "    mask_contour[:,:,1] = mask_contour[:,:,0]\n",
    "    mask_contour[:,:,2] = mask_contour[:,:,0]\n",
    "        \n",
    "    area_true[0,n_area] = np.sum(np.sum(mask[:,:,0]/255,0),0)/mask.shape[0]/mask.shape[1]\n",
    "    print(image_path)\n",
    "    print(\"true area: \", str(area_true[0,n_area]))   \n",
    "    \n",
    "    crop_size = 80\n",
    "    \n",
    "    x_image = get_patches_custom(\n",
    "        img_arr=image.astype(int), # required - array of images to be cropped\n",
    "        size=crop_size, # default is 256\n",
    "        stride=crop_size) # default is 256\n",
    "    \n",
    "    x_crops_image = torch.transpose(torch.from_numpy(x_image), 1, 3)/255\n",
    "    del(x_image)\n",
    "\n",
    "    len_dataset = x_crops_image.shape[0]\n",
    "    y_crops = np.zeros((len_dataset, 1))\n",
    "\n",
    "    testset = myDataset(data=x_crops_image.float(), targets=y_crops)\n",
    "    test_loader = DataLoader(testset, batch_size = 5000, shuffle = False)\n",
    "#     print('dataset is ready')\n",
    "    \n",
    "    ################################\n",
    "\n",
    "    k=0\n",
    "    predicted = np.zeros((len_dataset,1))\n",
    "    labels_true = predicted.copy()\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # print(images.shape)\n",
    "        # print(labels.shape)\n",
    "        outputs = model(images)\n",
    "        val, pred = torch.max(outputs.data, dim=1)\n",
    "        predicted[k*5000:(k+1)*5000] = pred.unsqueeze(1)\n",
    "    #     labels_true[k]=labels.squeeze(1)\n",
    "        k=k+1\n",
    "    #     print(k)\n",
    "\n",
    "    n_w = w_size//crop_size\n",
    "    n_h = h_size//crop_size\n",
    "\n",
    "    predicted_array = np.reshape(predicted, (n_w, n_h))\n",
    "    \n",
    "    for n in range(1, n_w-1):\n",
    "        for m in range(1, n_h-1):\n",
    "            if predicted_array[n-1,m-1]==0 and predicted_array[n-1,m]==0 and predicted_array[n,m-1]==0 and \\\n",
    "            predicted_array[n+1,m+1]==0 and  predicted_array[n+1,m]==0 and  predicted_array[n,m+1]==0 and \\\n",
    "            predicted_array[n-1,m+1]==0 and  predicted_array[n+1,m-1]==0 and   predicted_array[n,m]==1:\n",
    "\n",
    "                 predicted_array[n,m]=0 \n",
    "\n",
    "#             if predicted_array[n-1,m-1]==1 and predicted_array[n-1,m]==1 and predicted_array[n,m-1]==1 and \\\n",
    "#             predicted_array[n+1,m+1]==1 and  predicted_array[n+1,m]==1 and  predicted_array[n,m+1]==1 and \\\n",
    "#             predicted_array[n-1,m+1]==1 and  predicted_array[n+1,m-1]==1 and   predicted_array[n,m]==0:\n",
    "\n",
    "#                  predicted_array[n,m]=1\n",
    "    \n",
    "    predicted_filtered = np.reshape(predicted_array, (len_dataset, 1))    \n",
    "    \n",
    "    area_part[0,n_area] = np.sum(predicted_filtered)/len(predicted_filtered)\n",
    "    print(\"predicted area: \", area_part[0,n_area])\n",
    "\n",
    "    x_crops = torch.transpose(x_crops_image, 1, 3).detach().numpy()\n",
    "    x_predicted = x_crops.copy()\n",
    "    print(x_predicted.shape)\n",
    "\n",
    "    red_square = np.zeros(np.shape(x_predicted[0,:,:,:]))\n",
    "    green_square = red_square.copy()\n",
    "\n",
    "    red_square[:,:,0] = 255 + red_square[:,:,0]\n",
    "    green_square[:,:,0] = 255 + green_square[:,:,0]\n",
    "\n",
    "\n",
    "    for k in range(len_dataset):\n",
    "        if  predicted_filtered[k]==1:\n",
    "            x_predicted[k,:,:,0] = np.minimum(x_crops[k,:,:,0]+100, red_square[:,:,0])\n",
    "\n",
    "    ##################################\n",
    "\n",
    "\n",
    "    x_reconstructed = reconstruct_from_patches(\n",
    "        img_arr=x_predicted, # required - array of cropped out images\n",
    "        org_img_size=(w_size, h_size), # required - original size of the image\n",
    "        stride=crop_size) # use only if stride is different from patch size\n",
    "\n",
    "    print(\"x_reconstructed shape: \", str(x_reconstructed.shape))\n",
    "    segmented_image = np.minimum(x_reconstructed[0, 0:-1,0:-1,:], mask_contour[:,:,0:3]) #[0, 0:256*31+1, 0:256*31+1,:])\n",
    "\n",
    "    fig = plt.figure(figsize=(50,50)) \n",
    "    plt.axis('off')\n",
    "    plt.imshow(segmented_image) #x_reconstructed[0]) #[0, 0:256*31+1, 0:256*31+1,:])\n",
    "    plt.show()\n",
    "\n",
    "    image_name = image_path[0:len(image_path)-5]\n",
    "    #path where you can save figures\n",
    "    path = str(r\"C:\\Users\\adoro\\Documents\\AI\\test/\"+image_name+'_'+str(round(area_part[0,n_area],3))+'_crop80_5000_5000.png')\n",
    "    fig.savefig(path,  bbox_inches='tight')\n",
    "    n_area = n_area+1\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(area_part.T)\n",
    "print(area_true.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
